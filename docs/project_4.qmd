---
title: "Client Report - Project 4: Can you predict that?"
subtitle: "Course DS 250"
author: "Max Smith"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---

```{python}
#| label: libraries
#| include: false
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier
from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score


```


## Elevator pitch

This project takes on a machine learning model with the goal of predicting wether houses in Denver, Colorado, were built before or after 1980, with an accuracy rating of 90%. In the analysis, we focused on features like living area, number of bedrooms and bathrooms, and stories are significant points of focus between the change in house designs before and after 1980. I then used a couple of evaluation methods to validate the effectiveness of the Random Forest classifier used for this project!

```{python}
#| label: project-data
#| code-summary: Read and format project data

# Learn morea about Code Cells: https://quarto.org/docs/reference/cells/cells-jupyter.html

# Include and execute your code here
df = pd.read_csv('/Users/maxsmith/Desktop/DS 250/week-8/dwellings_ml.csv')
```

__Highlight the Questions and Tasks__

## QUESTION|TASK 1

Create 2-3 charts that evaluate potential relationships between the home variables and before1980. Explain what you learn from the charts that could help a machine learning algorithm.


```{python}
#| label: Q1
#| code-summary: Read and format data
# Include and execute your code here

# df.drop('yrbuilt')
```

In this first chart we can see that before 1980 the living area is pretty dense at 2404 sq ft, while after 1980 there is more of a spread. Although the tail of the violin chart is highest before 1980, that is just because of that one point skewing the data. For the most part before 1980 is more dense and after 1980 is more spread, but they are both pretty similar.

```{python}
#| label: Q1a-chart
#| code-summary: plot example
#| fig-cap: "Distribution of Living Area by Year Built Category"
#| fig-align: center
# Include and execute your code here
chart = px.violin(df,
  x="before1980", 
  y="livearea",
  color='before1980',
  box=True
)

chart.show()
```

In this graph we see that after 1980 has a much larger spread in netprice of homes than before 1980. Before, there were many houses around 600k, and after 1980 there are now a lot more houses above a million.

```{python}
#| label: Q1b-chart
#| code-summary: plot example
#| fig-cap: "Comparison of Net Price for Houses Based on Year Built"
#| fig-align: center
# Include and execute your code here
chart = px.violin(df,
  x="before1980", 
  y="netprice",
  color='before1980'
)

chart.show()
```

This graph shows us that there seems to be more homes in the data set before 1980 than after, but also that the majority of 2 bedroom homes are from before 1980. For both before and after most homes are between 1-4 rooms, primarily 2-3. 

```{python}
#| label: Q1c-chart
#| code-summary: plot example
#| fig-cap: "Number of Bedrooms Distribution Across Construction Eras"
#| fig-align: center
# Include and execute your code here

chart = px.histogram(df, 
  x="numbdrm", 
  color="before1980", 
  barmode="group", 
  title="Distribution of Bedroom Count by Construction Era")

chart.show()

```


## QUESTION|TASK 2

Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach or exceed 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.

I was able to reach 90% with an occasional 89% through the random forrest classifier. I started with the GaussianNB classifier and I couldn't get better than a 76%. I had messed around with adding and dropping features to see which ones helped. Then with the random forest, I started at 82% and messing with the test size I got it up to 87%, and finally tweaked the n estimators to be 200 which brought the accuracy to 90%.

```{python}
#| label: Q2
#| code-summary: Read and format data
# Include and execute your code here

# Select features that might influence the construction year of the house
features = df[['livearea', 'basement', 'stories', 'nocars', 'numbdrm', 'numbaths']]

# Target variable
target = df['before1980']

# Split the dataset into training and testing sets
train_features, test_features, train_target, test_target = train_test_split(features, target, test_size=0.2)

classifier = RandomForestClassifier(n_estimators=200)

# Train the classifier
classifier.fit(train_features, train_target)

# Make predictions on the test set
predictions = classifier.predict(test_features)

# Calculate the accuracy
accuracy = accuracy_score(test_target, predictions)
print(f'Random Forest Accuracy: {accuracy:.5f}')
```


## QUESTION|TASK 3

Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.

As this graph clearly shows, the living area more than doubles the next (basement) importance by more than double. This shows us how important each of these features are for our model and testing. I found it interesting th number of cars was important at all, even more so, not being last. This shows us that the biggest difference from homes before and after 1980 is the size of the living area.
```{python}
#| label: Q3
#| code-summary: Read and format data
# Include and execute your code here
feature_importances = pd.DataFrame(classifier.feature_importances_,
  index=features.columns,
  columns=['importance']).sort_values('importance', ascending=False)

# Reset index to get feature names into a column
feature_importances.reset_index(inplace=True)
feature_importances.rename(columns={'index': 'feature'}, inplace=True)

```


```{python}
#| label: Q3-chart
#| code-summary: plot example
#| fig-cap: "My useless chart"
#| fig-align: center
# Include and execute your code here
chart = px.bar(feature_importances,
  x='importance',
  y='feature',
  orientation='h',
  title='Feature Importance for Predicting Construction Year',
  labels={'feature': 'Feature', 'importance': 'Importance Score'},
  height=600,
  color='importance',
  color_continuous_scale=px.colors.sequential.Viridis)
# order it
chart.update_layout(yaxis={'categoryorder': 'total ascending'})

chart.show()
```

## QUESTION|TASK 4

Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.

The evaluation methods I chose are accuracy, confusion matrix, and precision. Accuracy is pretty straightforward, basically with an accuracy of .90, it means that 9/10 times our answer wil be right. The Confusion Matrix shows us exactly where our predictions land. It tells us the model correctly predicted "before 1980" for 1490 houses and "after 1980" for 2622 houses. It also shows where it got it wrong, with 231 mistakes for "before 1980" and 240 for "after 1980". Essentially, it breaks down the hits and misses, making it clear where the model excels or slips up. Prediction is the 'Ability of a model to identify only the relevant data points.' Our score for prediction was 92% which is very high!!

```{python}
#| label: Q4
#| code-summary: Read and format data
# Include and execute your code here

# Accuracy
accuracy = accuracy_score(test_target, predictions)
print(f'Accuracy: {accuracy:.2f}')

# Confusion Matrix
conf_matrix = confusion_matrix(test_target, predictions)
print(f'Confusion Matrix:\n{conf_matrix}')

# Precision
precision = precision_score(test_target, predictions)
print(f'Precision: {precision:.2f}')

```


